{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM8cP76KKxYrB9/5lakjDIY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anshulsinghkamboj-ml/nlp-/blob/main/transfromers_self_attaentiom.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "LKleQijGhmtL"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import math\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class TinySelfAttention(nn.Module):\n",
        "  def __init__(self,embed_dim):\n",
        "    super().__init__()\n",
        "    self.embed_dim=embed_dim\n",
        "    self.W_q=nn.Linear(embed_dim,embed_dim)\n",
        "    self.W_k=nn.Linear(embed_dim,embed_dim)\n",
        "    self.W_v=nn.Linear(embed_dim,embed_dim)\n",
        "\n",
        "  def forward(self,x):\n",
        "    q=self.W_q(x)\n",
        "    k=self.W_k(x)\n",
        "    v=self.W_v(x)\n",
        "\n",
        "    scores = q@k.transpose(-2,-1)/math.sqrt(self.embed_dim)\n",
        "    weights=torch.softmax(scores,dim=-1)\n",
        "    output=torch.matmul(weights,v)\n",
        "\n",
        "    return output,weights\n",
        "\n"
      ],
      "metadata": {
        "id": "DOpmnYeliZoy"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# demo\n",
        "x = torch.randn(1, 4, 8)  # batch=1, seq_len=4, embed_dim=8\n",
        "attn = TinySelfAttention(embed_dim=8)\n",
        "out, w = attn(x)\n",
        "print(out.shape)  # (1,4,8)\n",
        "print(w.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NFsCn6WGvH9I",
        "outputId": "9512978d-ad7d-4f71-e029-e83c77a53cb7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 4, 8])\n",
            "torch.Size([1, 4, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TinyMultiHead(nn.Module):\n",
        "  def __init__(self,embed_dim,num_heads):\n",
        "    super().__init__()\n",
        "    self.embed_dim=embed_dim\n",
        "    self.num_heads=num_heads\n",
        "    self.head_dim=embed_dim//num_heads\n",
        "\n",
        "    self.W_q=nn.Linear(embed_dim,embed_dim)\n",
        "    self.W_k=nn.Linear(embed_dim,embed_dim)\n",
        "    self.W_v=nn.Linear(embed_dim,embed_dim)\n",
        "\n",
        "    self.out = nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "  def head_split(self,x):\n",
        "    B,T,C=x.size()\n",
        "    x=x.view(B,T,self.num_heads,self.head_dim)\n",
        "    return x.transpose(1,2)\n",
        "\n",
        "  def combine_heads(self,x):\n",
        "    B, H, T, D = x.size()\n",
        "    x = x.transpose(1, 2).contiguous()\n",
        "    return x.view(B, T, H * D)\n",
        "\n",
        "  def forward(self,x):\n",
        "    Q=self.head_split(self.W_q(x))\n",
        "    K=self.head_split(self.W_k(x))\n",
        "    V=self.head_split(self.W_v(x))\n",
        "\n",
        "    scores=Q@K.transpose(-2,-1)/math.sqrt(self.head_dim)\n",
        "    weights=torch.softmax(scores,dim=-1)\n",
        "\n",
        "    out=weights@V\n",
        "\n",
        "    out=self.combine_heads(out)\n",
        "    out=self.out(out)\n",
        "\n",
        "    return out,weights\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "f1LwSzNOzIap"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.randn(1, 4, 8)  # batch=1, seq_len=4, embed_dim=8\n",
        "attn = TinyMultiHead(embed_dim=8,num_heads=2)\n",
        "out, w = attn(x)\n",
        "print(out.shape)  # (1,4,8)\n",
        "print(w.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6C7y13m-Ol8N",
        "outputId": "8d6f3b68-45aa-40a6-b034-1164505f97cb"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 4, 8])\n",
            "torch.Size([1, 2, 4, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TinyFeedForward(nn.Module):\n",
        "    def __init__(self, embed_dim, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(embed_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        return self.fc2(x)"
      ],
      "metadata": {
        "id": "tSQ2f99COpaf"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TinyTransformerBlock(nn.Module):\n",
        "  def __init__(self, embed_dim, num_heads, ff_hidden_dim):\n",
        "        super().__init__()\n",
        "        self.mha=TinyMultiHead(embed_dim,num_heads)\n",
        "        self.ff=TinyFeedForward(embed_dim, ff_hidden_dim)\n",
        "\n",
        "        self.ln1=nn.LayerNorm(embed_dim)\n",
        "        self.ln1=nn.LayerNorm(embed_dim)\n",
        "\n",
        "  def forward(self,x):\n",
        "    attn_out, weights = self.mha(x)\n",
        "    x = self.ln1(x + attn_out)\n",
        "    ff_out = self.ff(x)\n",
        "    x = self.ln2(x + ff_out)\n",
        "    return x, weights"
      ],
      "metadata": {
        "id": "EUdryjeYPLpH"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TinyPositionalEmbedding(nn.Module):\n",
        "    def __init__(self, max_len, embed_dim):\n",
        "        super().__init__()\n",
        "        self.pos_embedding = nn.Embedding(max_len, embed_dim)\n",
        "\n",
        "    def forward(self, seq_len):\n",
        "        # positions: [0, 1, 2, ..., seq_len-1]\n",
        "        positions = torch.arange(seq_len).unsqueeze(0)  # (1, T)\n",
        "        return self.pos_embedding(positions)            # (1, T, embed_dim)\n"
      ],
      "metadata": {
        "id": "zrnApOFlT5lE"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TinyInputEmbeddings(nn.Module):\n",
        "    def __init__(self, vocab_size, max_len, embed_dim):\n",
        "        super().__init__()\n",
        "        self.token_embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.pos_embedding   = nn.Embedding(max_len, embed_dim)\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        # input_ids: (B, T)\n",
        "        B, T = input_ids.shape\n",
        "\n",
        "        token_embeds = self.token_embedding(input_ids)   # (B, T, D)\n",
        "\n",
        "        positions = torch.arange(T, device=input_ids.device).unsqueeze(0)  # (1, T)\n",
        "        pos_embeds = self.pos_embedding(positions)       # (1, T, D)\n",
        "\n",
        "        return token_embeds + pos_embeds\n"
      ],
      "metadata": {
        "id": "O-YQ2qnFZN7Q"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TinyLMHead(nn.Module):\n",
        "    def __init__(self, embed_dim, vocab_size):\n",
        "        super().__init__()\n",
        "        self.fc = nn.Linear(embed_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (B, T, D)\n",
        "        return self.fc(x)  # (B, T, vocab_size)\n"
      ],
      "metadata": {
        "id": "zGE8l3yVafYt"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TinyMultiHead(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads):\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "\n",
        "        self.W_q = nn.Linear(embed_dim, embed_dim)\n",
        "        self.W_k = nn.Linear(embed_dim, embed_dim)\n",
        "        self.W_v = nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "        self.out = nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "    def split_heads(self, x):\n",
        "        B, T, C = x.size()\n",
        "        x = x.view(B, T, self.num_heads, self.head_dim)\n",
        "        return x.transpose(1, 2)\n",
        "\n",
        "    def combine_heads(self, x):\n",
        "        B, H, T, D = x.size()\n",
        "        x = x.transpose(1, 2).contiguous()\n",
        "        return x.view(B, T, H * D)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, _ = x.size()\n",
        "\n",
        "        Q = self.split_heads(self.W_q(x))\n",
        "        K = self.split_heads(self.W_k(x))\n",
        "        V = self.split_heads(self.W_v(x))\n",
        "\n",
        "        scores = Q @ K.transpose(-2, -1) / (self.head_dim ** 0.5)\n",
        "\n",
        "        # ---- CAUSAL MASK HERE ----\n",
        "        mask = torch.triu(torch.ones(T, T, device=x.device), diagonal=1).bool()\n",
        "        scores = scores.masked_fill(mask, float('-inf'))\n",
        "        # ---------------------------\n",
        "\n",
        "        weights = torch.softmax(scores, dim=-1)\n",
        "        out = weights @ V\n",
        "        out = self.combine_heads(out)\n",
        "\n",
        "        return self.out(out), weights\n"
      ],
      "metadata": {
        "id": "L1OGyi0tjhcM"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class TinyGPT(nn.Module):\n",
        "    def __init__(self, vocab_size, max_len, embed_dim, num_heads, ff_hidden_dim, num_layers):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embed = TinyInputEmbeddings(vocab_size, max_len, embed_dim)\n",
        "\n",
        "        self.layers = nn.ModuleList([\n",
        "            TinyTransformerBlock(embed_dim, num_heads, ff_hidden_dim)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        self.lm_head = TinyLMHead(embed_dim, vocab_size)\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        # input_ids: (B, T)\n",
        "        x = self.embed(input_ids)  # (B, T, D)\n",
        "\n",
        "        attn_maps = []\n",
        "        for layer in self.layers:\n",
        "            x, weights = layer(x)\n",
        "            attn_maps.append(weights)\n",
        "\n",
        "        logits = self.lm_head(x)  # (B, T, vocab_size)\n",
        "        return logits, attn_maps\n"
      ],
      "metadata": {
        "id": "aq0ZXtLFjTlR"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "\n",
        "# ---------------------------------------------\n",
        "# Multi-Head Attention (with causal mask)\n",
        "# ---------------------------------------------\n",
        "class TinyMultiHead(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads):\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "\n",
        "        self.W_q = nn.Linear(embed_dim, embed_dim)\n",
        "        self.W_k = nn.Linear(embed_dim, embed_dim)\n",
        "        self.W_v = nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "        self.out = nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "    def split_heads(self, x):\n",
        "        B, T, C = x.size()\n",
        "        x = x.view(B, T, self.num_heads, self.head_dim)\n",
        "        return x.transpose(1, 2)  # (B, H, T, D)\n",
        "\n",
        "    def combine_heads(self, x):\n",
        "        B, H, T, D = x.size()\n",
        "        x = x.transpose(1, 2).contiguous()\n",
        "        return x.view(B, T, H * D)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, _ = x.size()\n",
        "\n",
        "        Q = self.split_heads(self.W_q(x))\n",
        "        K = self.split_heads(self.W_k(x))\n",
        "        V = self.split_heads(self.W_v(x))\n",
        "\n",
        "        # Attention scores\n",
        "        scores = Q @ K.transpose(-2, -1) / math.sqrt(self.head_dim)\n",
        "\n",
        "        # Causal mask: prevent looking ahead\n",
        "        mask = torch.triu(torch.ones(T, T, device=x.device), diagonal=1).bool()\n",
        "        scores = scores.masked_fill(mask, float('-inf'))\n",
        "\n",
        "        weights = F.softmax(scores, dim=-1)\n",
        "        out = weights @ V\n",
        "        out = self.combine_heads(out)\n",
        "        return self.out(out), weights\n",
        "\n",
        "\n",
        "# ---------------------------------------------\n",
        "# Feed-Forward Network\n",
        "# ---------------------------------------------\n",
        "class TinyFeedForward(nn.Module):\n",
        "    def __init__(self, embed_dim, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(embed_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc2(F.relu(self.fc1(x)))\n",
        "\n",
        "\n",
        "# ---------------------------------------------\n",
        "# Transformer Block\n",
        "# ---------------------------------------------\n",
        "class TinyTransformerBlock(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, ff_hidden_dim):\n",
        "        super().__init__()\n",
        "        self.mha = TinyMultiHead(embed_dim, num_heads)\n",
        "        self.ff = TinyFeedForward(embed_dim, ff_hidden_dim)\n",
        "        self.ln1 = nn.LayerNorm(embed_dim)\n",
        "        self.ln2 = nn.LayerNorm(embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        attn_out, weights = self.mha(x)\n",
        "        x = self.ln1(x + attn_out)\n",
        "        ff_out = self.ff(x)\n",
        "        x = self.ln2(x + ff_out)\n",
        "        return x, weights\n",
        "\n",
        "\n",
        "# ---------------------------------------------\n",
        "# Token + Positional Embeddings\n",
        "# ---------------------------------------------\n",
        "class TinyInputEmbeddings(nn.Module):\n",
        "    def __init__(self, vocab_size, max_len, embed_dim):\n",
        "        super().__init__()\n",
        "        self.token_embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.pos_embedding = nn.Embedding(max_len, embed_dim)\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        B, T = input_ids.shape\n",
        "        token_embeds = self.token_embedding(input_ids)\n",
        "        positions = torch.arange(T, device=input_ids.device).unsqueeze(0)\n",
        "        pos_embeds = self.pos_embedding(positions)\n",
        "        return token_embeds + pos_embeds\n",
        "\n",
        "\n",
        "# ---------------------------------------------\n",
        "# LM Head\n",
        "# ---------------------------------------------\n",
        "class TinyLMHead(nn.Module):\n",
        "    def __init__(self, embed_dim, vocab_size):\n",
        "        super().__init__()\n",
        "        self.fc = nn.Linear(embed_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)\n",
        "\n",
        "\n",
        "# ---------------------------------------------\n",
        "# Full TinyGPT Model\n",
        "# ---------------------------------------------\n",
        "class TinyGPT(nn.Module):\n",
        "    def __init__(self, vocab_size, max_len, embed_dim, num_heads, ff_hidden_dim, num_layers):\n",
        "        super().__init__()\n",
        "        self.embed = TinyInputEmbeddings(vocab_size, max_len, embed_dim)\n",
        "\n",
        "        self.layers = nn.ModuleList([\n",
        "            TinyTransformerBlock(embed_dim, num_heads, ff_hidden_dim)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        self.lm_head = TinyLMHead(embed_dim, vocab_size)\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        x = self.embed(input_ids)\n",
        "        attn_maps = []\n",
        "\n",
        "        for layer in self.layers:\n",
        "            x, attn = layer(x)\n",
        "            attn_maps.append(attn)\n",
        "\n",
        "        logits = self.lm_head(x)\n",
        "        return logits, attn_maps\n"
      ],
      "metadata": {
        "id": "k_6GYmjakZZS"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------------------------------\n",
        "# Training Setup\n",
        "# ----------------------------------------------------\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "vocab_size = 100\n",
        "max_len = 32\n",
        "embed_dim = 64\n",
        "num_heads = 4\n",
        "ff_hidden_dim = 128\n",
        "num_layers = 2\n",
        "\n",
        "model = TinyGPT(vocab_size, max_len, embed_dim, num_heads, ff_hidden_dim, num_layers).to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Dummy dataset (replace with your tokenized text)\n",
        "batch_size = 16\n",
        "num_batches = 2000\n",
        "\n",
        "for step in range(num_batches):\n",
        "    input_ids = torch.randint(0, vocab_size, (batch_size, max_len)).to(device)\n",
        "    target_ids = input_ids.clone()\n",
        "\n",
        "    logits, _ = model(input_ids)\n",
        "\n",
        "    B, T, V = logits.shape\n",
        "    loss = criterion(logits.view(B*T, V), target_ids.view(B*T))\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if step % 100 == 0:\n",
        "        print(f\"Step {step} — Loss: {loss.item():.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xLbtFdiskxcD",
        "outputId": "95c346c2-f8db-4819-881b-ad4e56984f72"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 0 — Loss: 4.7583\n",
            "Step 100 — Loss: 2.7312\n",
            "Step 200 — Loss: 0.9432\n",
            "Step 300 — Loss: 0.3406\n",
            "Step 400 — Loss: 0.1615\n",
            "Step 500 — Loss: 0.0940\n",
            "Step 600 — Loss: 0.0612\n",
            "Step 700 — Loss: 0.0443\n",
            "Step 800 — Loss: 0.0338\n",
            "Step 900 — Loss: 0.0263\n",
            "Step 1000 — Loss: 0.0211\n",
            "Step 1100 — Loss: 0.0174\n",
            "Step 1200 — Loss: 0.0146\n",
            "Step 1300 — Loss: 0.0125\n",
            "Step 1400 — Loss: 0.0106\n",
            "Step 1500 — Loss: 0.0092\n",
            "Step 1600 — Loss: 0.0082\n",
            "Step 1700 — Loss: 0.0072\n",
            "Step 1800 — Loss: 0.0063\n",
            "Step 1900 — Loss: 0.0057\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "@torch.no_grad()\n",
        "def generate(model, input_ids, max_new_tokens, temperature=1.0, top_k=None):\n",
        "    model.eval()\n",
        "    for _ in range(max_new_tokens):\n",
        "        # crop to max_len if sequence grows\n",
        "        input_condensed = input_ids[:, -model.embed.pos_embedding.num_embeddings:]\n",
        "\n",
        "        logits, _ = model(input_condensed)\n",
        "\n",
        "        # take last token's logits\n",
        "        last_logits = logits[:, -1, :] / temperature\n",
        "\n",
        "        # optional top-k filtering\n",
        "        if top_k is not None:\n",
        "            values, _ = torch.topk(last_logits, top_k)\n",
        "            min_val = values[:, -1].unsqueeze(-1)\n",
        "            last_logits = torch.where(last_logits < min_val, torch.full_like(last_logits, -1e10), last_logits)\n",
        "\n",
        "        probs = F.softmax(last_logits, dim=-1)\n",
        "\n",
        "        next_token = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "        input_ids = torch.cat([input_ids, next_token], dim=1)\n",
        "\n",
        "    return input_ids\n"
      ],
      "metadata": {
        "id": "aj85fhdulGbm"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start = torch.tensor([[12]], device=device)  # starting token\n",
        "output = generate(model, start, max_new_tokens=30, temperature=1.0, top_k=5)\n",
        "print(output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u5QnjyWIlZBz",
        "outputId": "1a6b267d-19cd-4bda-d0fe-6d914c5556a7"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
            "         12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12]], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"harry1.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    text = f.read()"
      ],
      "metadata": {
        "id": "F9GC-d30lalY"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "\n",
        "stoi = {ch:i for i,ch in enumerate(chars)}\n",
        "itos = {i:ch for i,ch in enumerate(chars)}\n",
        "\n",
        "def encode(s):\n",
        "    return [stoi[c] for c in s]\n",
        "\n",
        "def decode(tokens):\n",
        "    return ''.join(itos[t] for t in tokens)\n"
      ],
      "metadata": {
        "id": "KHNhhRWomEyM"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "print(\"Dataset tokens:\", len(data))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Icq3-MpzmHII",
        "outputId": "c2455051-92a6-4829-e2c0-a61285566834"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset tokens: 439478\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n = int(0.9 * len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n"
      ],
      "metadata": {
        "id": "aJ0ZTsBPmI8d"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_batch(split, batch_size=32, block_size=128):\n",
        "    data_source = train_data if split == \"train\" else val_data\n",
        "    ix = torch.randint(0, len(data_source) - block_size - 1, (batch_size,))\n",
        "\n",
        "    x = torch.stack([data_source[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data_source[i+1:i+block_size+1] for i in ix])\n",
        "\n",
        "    return x.to(device), y.to(device)\n"
      ],
      "metadata": {
        "id": "Tx0w6RR5mKdr"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embed_dim = 128\n",
        "num_heads = 4\n",
        "ff_hidden_dim = 256\n",
        "num_layers = 4\n",
        "max_len = 128\n",
        "\n",
        "model = TinyGPT(vocab_size, max_len, embed_dim, num_heads, ff_hidden_dim, num_layers).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
        "criterion = nn.CrossEntropyLoss()\n"
      ],
      "metadata": {
        "id": "BlVQxy_VmL2g"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "steps = 10000\n",
        "batch_size = 32\n",
        "block_size = max_len\n",
        "\n",
        "for step in range(steps):\n",
        "    x, y = get_batch(\"train\", batch_size, block_size)\n",
        "\n",
        "    logits, _ = model(x)\n",
        "\n",
        "    B, T, V = logits.shape\n",
        "    loss = criterion(logits.view(B*T, V), y.view(B*T))\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Log every so often\n",
        "    if step % 200 == 0:\n",
        "        val_x, val_y = get_batch(\"val\", batch_size, block_size)\n",
        "        with torch.no_grad():\n",
        "            val_logits, _ = model(val_x)\n",
        "            val_loss = criterion(val_logits.view(B*T, V), val_y.view(B*T))\n",
        "        print(f\"Step {step} — train loss: {loss.item():.4f} — val loss: {val_loss.item():.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xi63m58rmNM9",
        "outputId": "6ffd4e08-0c03-4b10-ee62-4bbcfb5d95aa"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 0 — train loss: 1.4981 — val loss: 1.5019\n",
            "Step 200 — train loss: 1.4176 — val loss: 1.5171\n",
            "Step 400 — train loss: 1.4386 — val loss: 1.5397\n",
            "Step 600 — train loss: 1.4415 — val loss: 1.4658\n",
            "Step 800 — train loss: 1.4068 — val loss: 1.4790\n",
            "Step 1000 — train loss: 1.3528 — val loss: 1.4826\n",
            "Step 1200 — train loss: 1.4247 — val loss: 1.5074\n",
            "Step 1400 — train loss: 1.3136 — val loss: 1.4883\n",
            "Step 1600 — train loss: 1.2862 — val loss: 1.4013\n",
            "Step 1800 — train loss: 1.3701 — val loss: 1.4568\n",
            "Step 2000 — train loss: 1.3063 — val loss: 1.4039\n",
            "Step 2200 — train loss: 1.3103 — val loss: 1.3655\n",
            "Step 2400 — train loss: 1.3174 — val loss: 1.3804\n",
            "Step 2600 — train loss: 1.2595 — val loss: 1.4321\n",
            "Step 2800 — train loss: 1.2989 — val loss: 1.3804\n",
            "Step 3000 — train loss: 1.2845 — val loss: 1.4256\n",
            "Step 3200 — train loss: 1.3113 — val loss: 1.4051\n",
            "Step 3400 — train loss: 1.2953 — val loss: 1.4032\n",
            "Step 3600 — train loss: 1.2877 — val loss: 1.3685\n",
            "Step 3800 — train loss: 1.2231 — val loss: 1.4272\n",
            "Step 4000 — train loss: 1.2144 — val loss: 1.3655\n",
            "Step 4200 — train loss: 1.1737 — val loss: 1.3676\n",
            "Step 4400 — train loss: 1.2354 — val loss: 1.3701\n",
            "Step 4600 — train loss: 1.2405 — val loss: 1.3804\n",
            "Step 4800 — train loss: 1.1678 — val loss: 1.3041\n",
            "Step 5000 — train loss: 1.2196 — val loss: 1.4012\n",
            "Step 5200 — train loss: 1.1183 — val loss: 1.4378\n",
            "Step 5400 — train loss: 1.1457 — val loss: 1.3581\n",
            "Step 5600 — train loss: 1.2062 — val loss: 1.3924\n",
            "Step 5800 — train loss: 1.2015 — val loss: 1.4145\n",
            "Step 6000 — train loss: 1.1528 — val loss: 1.3486\n",
            "Step 6200 — train loss: 1.0842 — val loss: 1.3575\n",
            "Step 6400 — train loss: 1.1640 — val loss: 1.4034\n",
            "Step 6600 — train loss: 1.1984 — val loss: 1.3798\n",
            "Step 6800 — train loss: 1.1161 — val loss: 1.3356\n",
            "Step 7000 — train loss: 1.1076 — val loss: 1.4440\n",
            "Step 7200 — train loss: 1.1510 — val loss: 1.4145\n",
            "Step 7400 — train loss: 1.1450 — val loss: 1.3906\n",
            "Step 7600 — train loss: 1.1476 — val loss: 1.3294\n",
            "Step 7800 — train loss: 1.1007 — val loss: 1.4260\n",
            "Step 8000 — train loss: 1.0678 — val loss: 1.3267\n",
            "Step 8200 — train loss: 1.1294 — val loss: 1.3920\n",
            "Step 8400 — train loss: 1.0922 — val loss: 1.3829\n",
            "Step 8600 — train loss: 1.0619 — val loss: 1.3324\n",
            "Step 8800 — train loss: 1.0735 — val loss: 1.3652\n",
            "Step 9000 — train loss: 1.1046 — val loss: 1.4006\n",
            "Step 9200 — train loss: 1.1457 — val loss: 1.3955\n",
            "Step 9400 — train loss: 1.1367 — val loss: 1.3414\n",
            "Step 9600 — train loss: 1.0375 — val loss: 1.4090\n",
            "Step 9800 — train loss: 1.0458 — val loss: 1.3759\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "start = torch.tensor([[stoi['T']]], device=device)  # any starting character\n",
        "output = generate(model, start, max_new_tokens=500, temperature=0.8, top_k=20)\n",
        "print(decode(output[0].tolist()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SPLzo9i5mRgK",
        "outputId": "642f79ca-2e4d-4bdc-ed0f-9fa94e37d6b1"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The neast dream, Harry thought broke his head book as the whole are he was out in a bald ball watter full out how to the twin near window. “You could take you think they:\n",
            "\n",
            "One thought was Snape say to sleep you, to have a corner leave at all.\n",
            "\n",
            "“Yes, he’s but what we’re just enough to think all right,” said Hagrid chains are waved and something….an…this he had always little back at them, nothing lebes noise at the points wizard with the house cheers.\n",
            "\n",
            "“His arms and to find out what you got a bear \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LrxcyWRcmUnc"
      },
      "execution_count": 24,
      "outputs": []
    }
  ]
}